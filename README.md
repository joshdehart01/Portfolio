# Portfolio
Welcome to my Data Analyst Portfolio!

I am currently pursuing an Information Technology degree with a focus in Data Analytics from Texas Tech University. This program has prepared me with a great set of tools and knowledge for my December 2024 graduation. During my studies, I have sharpened my ability to spot trends and gain insights from data sets. I have a passion for dissecting data, complimented by a curious mind. Prior to switching to IT, I was an engineering student, which has helped me bring a unique problem solving mindset to projects and discussions. In my free time, I am always looking for new ways to learn and master my skills. I am thrilled to bring my analytical abilities into the field of data!

Within this portfolio, I showcase my skills including Excel, Python, SQL, and Tableau.

Here is a description of each project:


<h2>Movie Success Correlation<h2>
<h4>Goal:</h4>
Determine which factors contribute to a movie's gross profit.
<h4>Description:</h4>
This project is centered around attributes of movies ranging from 1980-2020. The data set includes name, year, genre, votes, director, star, budget, gross profit, company, and other important fields. This project required the cleaning of data, basic statistical operations, and a Pearson correlation analysis.
<h4>Skills Used:</h4>
Data cleaning, Data analysis, Correlation matrix, Data visualization
<h4>Technology:</h4>
Python, Pandas, Seaborn, Numpy, Matplotlib
<h4>Results:</h4>
After running analysis and the Pearson correlation, we learn that the budget and number of user votes contribute the most to the success of a movie.


<h2>Athlete Web Scrape<h2>
<h4>Goal:</h4>
Scrape site and export data to a CSV for future analysis.
<h4>Description:</h4>
This project revolves around scraping a site that contains information about athletes. There is a parent page that lists an athlete's name, rank, and important metrics. For each athlete, there is a child page you can go to that provides further information including sleep, water consumed, and daily step count. To get the required information for the CSV, this project required a nested loop for each athlete to access the child pages.
<h4>Skills Used:</h4>
Web Scraping, Indexing, Throttling, Exporting data to CSV
<h4>Technology:</h4>
Python, BeautifulSoup, Requests Library, Random Library, CSV Library, Time Library
<h4>Results:</h4>
After accessing the webpage, scraping it, and extracting data, we now have a CSV that is ready for analysis.


<h2>COVID Data Exploration<h2>
<h4>Goal:</h4>
Run basic analysis on COVID data sets to get a variety of different information/statistics.
<h4>Description:</h4>
Using two different data sets containing information about COVID, we run basic calcuations to find results based off of geographical location. The first data set gives information about COVID deaths including country, date (updated daily), new cases, new deaths, and population. The second data set gives information about the COVID vaccines including country, date, new tests, new vaccines, and population. We join these data sets to get information on how the vaccine affected the number of cases and deaths by country/continent.
<h4>Skills Used:</h4>
Joins, CTE, Temp Tables, Aggregate Functions, Creating Views, Converting Data Types
<h4>Technology:</h4>
SQL Server
<h4>Results:</h4>
After joining the tables, we learn that new COVID cases slow down after the rollout of the vaccination.


<h2>COVID Dashboard<h2>
<h4>Goal:</h4>
Using the COVID data set that we ran analysis on in SQL, combine charts into a dashboard to visualize data.
<h4>Description:</h4>
After running basic calculations on the data set in SQL, we now move to Tableau to see our data in a variety of different charts. This will help us come to conclusions and present our findings to stakeholders.
<h4>Skills Used:</h4>
Creating Bar Charts, Tables, Line Charts, and Maps
<h4>Technology:</h4>
Tableau
<h4>Results:</h4>
After combining charts together to form a dashboard, we learn that the United States and certain areas of Europe struggled to deal with the spread of COVID.
<h4>Link:</h4>
Click <a href="https://public.tableau.com/app/profile/josh.dehart/viz/PortfolioCovidDashboard_17104313893560/COVIDDashboard" target="_blank">here</a> to view the dashboard.


<h2>Nashville Housing Data Cleaning<h2>
<h4>Goal:</h4>
Clean data set to prepare for analysis.
<h4>Description:</h4>
This project contains a Nashville Housing data set that shows the sales of homes between 2013-2019. There are null values, duplicates, columns that need to be split, and other problems that challenge this data's integrity. We be cleaning this data to have consistent and accurate data for analysis.
<h4>Skills Used:</h4>
Joins, Splitting Columns with SUBSTRING and PARSENAME, Updating Column Values, Removing Duplicates, Dropping Columns, CTE, Converting Data Types
<h4>Technology:</h4>
SQL Server
<h4>Results:</h4>
After cleaning the data, we now have a fresh and reliable data set that can be used for analysis.


<h2>Bike Dashboard<h2>
<h4>Goal:</h4>
Clean data set and create dashboard.
<h4>Description:</h4>
This project contains a data set involving different attributes that contribute to a person's decision to purchase a bike. We will clean this data set, and create charts that will combine together into a dashboard.
<h4>Skills Used:</h4>
Data Cleaning, PivotTables, Building Dashboard, Slicers, IF Function
<h4>Technology:</h4>
Microsoft Excel
<h4>Results:</h4>
After cleaning the data and creating a dashboard, we learn that middle-aged men with a short commute distance are the most likely to purchase a bike.
